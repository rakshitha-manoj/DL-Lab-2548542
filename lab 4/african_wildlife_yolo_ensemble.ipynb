{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶Å African Wildlife Detection: YOLOv5 + YOLOv8 + Ensemble Learning\n",
    "**Dataset:** [Ultralytics African Wildlife](https://docs.ultralytics.com/datasets/detect/african-wildlife/)  \n",
    "**Classes:** Buffalo, Elephant, Rhino, Zebra  \n",
    "**Goal:** Train YOLOv5 & YOLOv8, then combine them via Weighted Box Fusion ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Install Dependencies"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics ensemble-boxes supervision pycocotools matplotlib seaborn -q\n",
    "\n",
    "# Clone YOLOv5 repo (needed for its train.py)\n",
    "!git clone https://github.com/ultralytics/yolov5.git -q\n",
    "!pip install -r yolov5/requirements.txt -q\n",
    "\n",
    "print('‚úÖ All dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. Setup & Imports"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from ensemble_boxes import weighted_boxes_fusion\n",
    "\n",
    "print(f'PyTorch : {torch.__version__}')\n",
    "print(f'CUDA    : {torch.cuda.is_available()}')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device  : {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. Download the African Wildlife Dataset"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultralytics auto-downloads the dataset when you first call .train() or .val()\n",
    "# OR you can explicitly trigger it like this:\n",
    "from ultralytics.utils.downloads import download\n",
    "from ultralytics import settings\n",
    "\n",
    "# Check where ultralytics stores datasets\n",
    "DATASET_ROOT = Path(settings['datasets_dir'])\n",
    "DATASET_PATH = DATASET_ROOT / 'african-wildlife'\n",
    "print(f'Dataset will be stored at: {DATASET_PATH}')\n",
    "\n",
    "# Trigger download via a dummy val run (fastest way to get the dataset)\n",
    "if not DATASET_PATH.exists():\n",
    "    tmp = YOLO('yolov8n.pt')\n",
    "    tmp.val(data='african-wildlife.yaml', imgsz=640, verbose=False)\n",
    "    print('‚úÖ Dataset downloaded.')\n",
    "else:\n",
    "    print('‚úÖ Dataset already present.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 4. Explore the Dataset"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = {0: 'Buffalo', 1: 'Elephant', 2: 'Rhino', 3: 'Zebra'}\n",
    "COLORS  = {0: '#F5A623', 1: '#3B82F6', 2: '#A855F7', 3: '#22C55E'}\n",
    "\n",
    "splits = ['train', 'val', 'test']\n",
    "stats = {}\n",
    "\n",
    "for split in splits:\n",
    "    img_dir = DATASET_PATH / 'images' / split\n",
    "    lbl_dir = DATASET_PATH / 'labels' / split\n",
    "    imgs = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
    "    class_counts = {v: 0 for v in CLASSES.values()}\n",
    "    for lbl in lbl_dir.glob('*.txt'):\n",
    "        for line in lbl.read_text().strip().splitlines():\n",
    "            cls = int(line.split()[0])\n",
    "            class_counts[CLASSES[cls]] += 1\n",
    "    stats[split] = {'images': len(imgs), **class_counts}\n",
    "\n",
    "df = pd.DataFrame(stats).T\n",
    "print(df)\n",
    "\n",
    "# Bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.patch.set_facecolor('#0D1117')\n",
    "for ax in axes: ax.set_facecolor('#161B22')\n",
    "\n",
    "# Images per split\n",
    "axes[0].bar(df.index, df['images'], color=['#F5A623','#3B82F6','#22C55E'], edgecolor='none', width=0.5)\n",
    "axes[0].set_title('Images per Split', color='white', fontsize=13, fontweight='bold')\n",
    "axes[0].tick_params(colors='white')\n",
    "for spine in axes[0].spines.values(): spine.set_edgecolor('#30363D')\n",
    "axes[0].yaxis.label.set_color('white')\n",
    "\n",
    "# Class distribution (train)\n",
    "class_data = df.loc['train', list(CLASSES.values())]\n",
    "axes[1].bar(class_data.index, class_data.values,\n",
    "            color=[COLORS[i] for i in range(4)], edgecolor='none', width=0.5)\n",
    "axes[1].set_title('Class Distribution (Train)', color='white', fontsize=13, fontweight='bold')\n",
    "axes[1].tick_params(colors='white')\n",
    "for spine in axes[1].spines.values(): spine.set_edgecolor('#30363D')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dataset_stats.png', dpi=150, bbox_inches='tight', facecolor='#0D1117')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise a few training images with ground-truth boxes\n",
    "def plot_sample_images(n=6):\n",
    "    img_dir = DATASET_PATH / 'images' / 'train'\n",
    "    lbl_dir = DATASET_PATH / 'labels' / 'train'\n",
    "    img_files = sorted(img_dir.glob('*.jpg'))[:n]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    fig.patch.set_facecolor('#0D1117')\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, img_path in zip(axes, img_files):\n",
    "        img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        h, w = img.shape[:2]\n",
    "        lbl_path = lbl_dir / (img_path.stem + '.txt')\n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.set_facecolor('#161B22')\n",
    "        ax.axis('off')\n",
    "\n",
    "        if lbl_path.exists():\n",
    "            for line in lbl_path.read_text().strip().splitlines():\n",
    "                cls, cx, cy, bw, bh = map(float, line.split())\n",
    "                cls = int(cls)\n",
    "                x1 = (cx - bw/2) * w\n",
    "                y1 = (cy - bh/2) * h\n",
    "                rect = patches.Rectangle(\n",
    "                    (x1, y1), bw*w, bh*h,\n",
    "                    linewidth=2, edgecolor=COLORS[cls], facecolor='none'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x1, y1-5, CLASSES[cls],\n",
    "                        color='white', fontsize=8, fontweight='bold',\n",
    "                        bbox=dict(facecolor=COLORS[cls], alpha=0.8, pad=2, edgecolor='none'))\n",
    "\n",
    "    plt.suptitle('Sample Training Images', color='white', fontsize=15, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sample_images.png', dpi=150, bbox_inches='tight', facecolor='#0D1117')\n",
    "    plt.show()\n",
    "\n",
    "plot_sample_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 5. Train YOLOv5m"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv5 uses its own train.py script\n",
    "# We call it via subprocess\n",
    "import subprocess\n",
    "\n",
    "yolov5_cmd = [\n",
    "    'python', 'yolov5/train.py',\n",
    "    '--img',     '640',\n",
    "    '--batch',   '16',\n",
    "    '--epochs',  '100',\n",
    "    '--data',    'african-wildlife.yaml',\n",
    "    '--weights', 'yolov5m.pt',      # pretrained on COCO\n",
    "    '--project', 'runs/yolov5',\n",
    "    '--name',    'wildlife',\n",
    "    '--exist-ok',\n",
    "    '--cache',\n",
    "    '--device',  '0' if torch.cuda.is_available() else 'cpu',\n",
    "]\n",
    "\n",
    "print('Starting YOLOv5m training...')\n",
    "result = subprocess.run(yolov5_cmd, capture_output=False)\n",
    "\n",
    "V5_WEIGHTS = Path('runs/yolov5/wildlife/weights/best.pt')\n",
    "print(f'\\n‚úÖ YOLOv5m training complete. Best weights: {V5_WEIGHTS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 6. Train YOLOv8s"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v8 = YOLO('yolov8s.pt')   # pretrained on COCO\n",
    "\n",
    "results_v8 = model_v8.train(\n",
    "    data       = 'african-wildlife.yaml',\n",
    "    epochs     = 100,\n",
    "    imgsz      = 640,\n",
    "    batch      = 16,\n",
    "    lr0        = 0.01,\n",
    "    weight_decay = 0.0005,\n",
    "    augment    = True,\n",
    "    mosaic     = 1.0,\n",
    "    project    = 'runs/yolov8',\n",
    "    name       = 'wildlife',\n",
    "    exist_ok   = True,\n",
    "    device     = device,\n",
    "    verbose    = True,\n",
    ")\n",
    "\n",
    "V8_WEIGHTS = Path('runs/yolov8/wildlife/weights/best.pt')\n",
    "print(f'\\n‚úÖ YOLOv8s training complete. Best weights: {V8_WEIGHTS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 7. Plot Training Curves"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(csv_path, title, color):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    fig.patch.set_facecolor('#0D1117')\n",
    "    for ax in axes: ax.set_facecolor('#161B22')\n",
    "\n",
    "    metrics = [\n",
    "        ('metrics/mAP50(B)',      'mAP@0.5'),\n",
    "        ('metrics/precision(B)',  'Precision'),\n",
    "        ('metrics/recall(B)',     'Recall'),\n",
    "    ]\n",
    "\n",
    "    for ax, (col, label) in zip(axes, metrics):\n",
    "        if col in df.columns:\n",
    "            ax.plot(df['epoch'], df[col], color=color, linewidth=2)\n",
    "            ax.set_title(label, color='white', fontsize=11, fontweight='bold')\n",
    "            ax.set_xlabel('Epoch', color='#6B7280', fontsize=9)\n",
    "            ax.tick_params(colors='white')\n",
    "            ax.grid(alpha=0.1, color='white')\n",
    "            for spine in ax.spines.values(): spine.set_edgecolor('#30363D')\n",
    "\n",
    "    plt.suptitle(title, color='white', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title.replace(\" \",\"_\")}_curves.png', dpi=150,\n",
    "                bbox_inches='tight', facecolor='#0D1117')\n",
    "    plt.show()\n",
    "\n",
    "# YOLOv5 results CSV\n",
    "plot_training_curves('runs/yolov5/wildlife/results.csv',  'YOLOv5m Training', '#F5A623')\n",
    "\n",
    "# YOLOv8 results CSV\n",
    "plot_training_curves('runs/yolov8/wildlife/results.csv',  'YOLOv8s Training', '#3B82F6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 8. Evaluate Individual Models on Test Set"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weights\n",
    "model_v5 = YOLO(str(V5_WEIGHTS))\n",
    "model_v8 = YOLO(str(V8_WEIGHTS))\n",
    "\n",
    "print('=== YOLOv5m ‚Äî Test Set ===')\n",
    "metrics_v5 = model_v5.val(data='african-wildlife.yaml', split='test', verbose=True)\n",
    "\n",
    "print('\\n=== YOLOv8s ‚Äî Test Set ===')\n",
    "metrics_v8 = model_v8.val(data='african-wildlife.yaml', split='test', verbose=True)\n",
    "\n",
    "def extract_metrics(m, label):\n",
    "    return {\n",
    "        'Model':        label,\n",
    "        'mAP@0.5':      round(m.box.map50, 4),\n",
    "        'mAP@0.5:0.95': round(m.box.map,   4),\n",
    "        'Precision':    round(m.box.mp,     4),\n",
    "        'Recall':       round(m.box.mr,     4),\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame([\n",
    "    extract_metrics(metrics_v5, 'YOLOv5m'),\n",
    "    extract_metrics(metrics_v8, 'YOLOv8s'),\n",
    "])\n",
    "print('\\n', results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ensemble: Weighted Box Fusion (WBF)\n",
    "We combine predictions from both models at inference time using **Weighted Box Fusion**.\n",
    "- Boxes from both models are clustered by IoU overlap\n",
    "- Each cluster is fused into one box by weighted averaging coordinates and scores\n",
    "- Model agreement boosts confidence ‚Äî divergence dampens it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(model, image_path, conf=0.25):\n",
    "    \"\"\"Run one model and return normalised boxes, scores, labels.\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    h, w = img.shape[:2]\n",
    "    r = model.predict(str(image_path), conf=conf, verbose=False)[0]\n",
    "\n",
    "    if len(r.boxes) == 0:\n",
    "        return np.zeros((0,4)), np.zeros(0), np.zeros(0, dtype=int)\n",
    "\n",
    "    boxes  = r.boxes.xyxy.cpu().numpy().copy()\n",
    "    scores = r.boxes.conf.cpu().numpy()\n",
    "    labels = r.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    # Normalise to [0, 1] (required by WBF)\n",
    "    boxes[:, [0,2]] /= w\n",
    "    boxes[:, [1,3]] /= h\n",
    "    boxes = np.clip(boxes, 0, 1)\n",
    "    return boxes, scores, labels\n",
    "\n",
    "\n",
    "def ensemble_predict(image_path,\n",
    "                     models,\n",
    "                     weights=None,\n",
    "                     iou_thr=0.55,\n",
    "                     skip_thr=0.30,\n",
    "                     conf=0.25):\n",
    "    \"\"\"\n",
    "    Run WBF ensemble over a list of YOLO models.\n",
    "\n",
    "    Args:\n",
    "        image_path : path to image\n",
    "        models     : list of YOLO model objects\n",
    "        weights    : per-model weights (None = uniform)\n",
    "        iou_thr    : IoU threshold for clustering boxes\n",
    "        skip_thr   : boxes with fused score < this are dropped\n",
    "        conf       : detection confidence threshold per model\n",
    "\n",
    "    Returns:\n",
    "        fused_boxes  : (N,4) normalised xyxy\n",
    "        fused_scores : (N,)\n",
    "        fused_labels : (N,) int\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1.0] * len(models)\n",
    "\n",
    "    boxes_list, scores_list, labels_list = [], [], []\n",
    "\n",
    "    for model in models:\n",
    "        b, s, l = predict_single(model, image_path, conf=conf)\n",
    "        boxes_list.append(b.tolist())\n",
    "        scores_list.append(s.tolist())\n",
    "        labels_list.append(l.tolist())\n",
    "\n",
    "    fused_boxes, fused_scores, fused_labels = weighted_boxes_fusion(\n",
    "        boxes_list, scores_list, labels_list,\n",
    "        weights=weights,\n",
    "        iou_thr=iou_thr,\n",
    "        skip_box_thr=skip_thr,\n",
    "    )\n",
    "    return fused_boxes, fused_scores, fused_labels.astype(int)\n",
    "\n",
    "\n",
    "print('‚úÖ Ensemble functions ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 10. Evaluate Ensemble on Test Set (COCO-style mAP)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import itertools\n",
    "\n",
    "# ‚îÄ‚îÄ Convert YOLO labels to COCO-format GT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def build_coco_gt(img_dir, lbl_dir):\n",
    "    img_dir = Path(img_dir)\n",
    "    lbl_dir = Path(lbl_dir)\n",
    "    coco = {'images': [], 'annotations': [], 'categories': [\n",
    "        {'id': i, 'name': v} for i, v in CLASSES.items()\n",
    "    ]}\n",
    "    ann_id = 0\n",
    "    for img_id, img_path in enumerate(sorted(img_dir.glob('*.jpg'))):\n",
    "        img = Image.open(img_path)\n",
    "        w, h = img.size\n",
    "        coco['images'].append({'id': img_id, 'file_name': img_path.name,\n",
    "                               'width': w, 'height': h})\n",
    "        lbl_path = lbl_dir / (img_path.stem + '.txt')\n",
    "        if lbl_path.exists():\n",
    "            for line in lbl_path.read_text().strip().splitlines():\n",
    "                cls, cx, cy, bw, bh = map(float, line.split())\n",
    "                x1 = (cx - bw/2) * w\n",
    "                y1 = (cy - bh/2) * h\n",
    "                coco['annotations'].append({\n",
    "                    'id': ann_id, 'image_id': img_id,\n",
    "                    'category_id': int(cls),\n",
    "                    'bbox': [x1, y1, bw*w, bh*h],\n",
    "                    'area': bw*w * bh*h,\n",
    "                    'iscrowd': 0,\n",
    "                })\n",
    "                ann_id += 1\n",
    "    return coco\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Run ensemble on all test images & collect predictions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def evaluate_ensemble(models, weights, iou_thr=0.55, skip_thr=0.30):\n",
    "    test_img_dir = DATASET_PATH / 'images' / 'test'\n",
    "    test_lbl_dir = DATASET_PATH / 'labels' / 'test'\n",
    "\n",
    "    coco_gt_dict = build_coco_gt(test_img_dir, test_lbl_dir)\n",
    "\n",
    "    # Write GT to temp json\n",
    "    gt_path = '/tmp/coco_gt.json'\n",
    "    with open(gt_path, 'w') as f:\n",
    "        json.dump(coco_gt_dict, f)\n",
    "    coco_gt = COCO(gt_path)\n",
    "\n",
    "    img_list = sorted(test_img_dir.glob('*.jpg'))\n",
    "    img_name_to_id = {im['file_name']: im['id'] for im in coco_gt_dict['images']}\n",
    "\n",
    "    dt_list = []\n",
    "    for img_path in img_list:\n",
    "        img_id = img_name_to_id[img_path.name]\n",
    "        img = Image.open(img_path)\n",
    "        W, H = img.size\n",
    "\n",
    "        fboxes, fscores, flabels = ensemble_predict(\n",
    "            img_path, models, weights=weights,\n",
    "            iou_thr=iou_thr, skip_thr=skip_thr)\n",
    "\n",
    "        for box, score, label in zip(fboxes, fscores, flabels):\n",
    "            x1, y1, x2, y2 = box\n",
    "            dt_list.append({\n",
    "                'image_id':    img_id,\n",
    "                'category_id': int(label),\n",
    "                'bbox': [x1*W, y1*H, (x2-x1)*W, (y2-y1)*H],\n",
    "                'score': float(score),\n",
    "            })\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(dt_list) if dt_list else coco_gt.loadRes([])\n",
    "    ev = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    ev.evaluate(); ev.accumulate(); ev.summarize()\n",
    "    return ev.stats  # [mAP@0.5:0.95, mAP@0.5, ...]\n",
    "\n",
    "\n",
    "print('Running ensemble evaluation (this may take a few minutes)...')\n",
    "ensemble_stats = evaluate_ensemble(\n",
    "    models  = [model_v5, model_v8],\n",
    "    weights = [0.45, 0.55],   # slightly favour YOLOv8s\n",
    "    iou_thr = 0.55,\n",
    "    skip_thr= 0.30,\n",
    ")\n",
    "\n",
    "print(f'\\nEnsemble mAP@0.5      : {ensemble_stats[1]:.4f}')\n",
    "print(f'Ensemble mAP@0.5:0.95 : {ensemble_stats[0]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 11. Compare All Three: Single Models vs Ensemble"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison dataframe\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model':        'YOLOv5m',\n",
    "        'mAP@0.5':      metrics_v5.box.map50,\n",
    "        'mAP@0.5:0.95': metrics_v5.box.map,\n",
    "        'Precision':    metrics_v5.box.mp,\n",
    "        'Recall':       metrics_v5.box.mr,\n",
    "    },\n",
    "    {\n",
    "        'Model':        'YOLOv8s',\n",
    "        'mAP@0.5':      metrics_v8.box.map50,\n",
    "        'mAP@0.5:0.95': metrics_v8.box.map,\n",
    "        'Precision':    metrics_v8.box.mp,\n",
    "        'Recall':       metrics_v8.box.mr,\n",
    "    },\n",
    "    {\n",
    "        'Model':        'Ensemble (WBF)',\n",
    "        'mAP@0.5':      ensemble_stats[1],\n",
    "        'mAP@0.5:0.95': ensemble_stats[0],\n",
    "        'Precision':    None,   # COCO eval doesn't directly return P/R in same form\n",
    "        'Recall':       None,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(comparison.to_string(index=False, float_format='{:.4f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "metrics_to_plot = ['mAP@0.5', 'mAP@0.5:0.95']\n",
    "bar_colors = ['#F5A623', '#3B82F6', '#22C55E']\n",
    "models_labels = comparison['Model'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.patch.set_facecolor('#0D1117')\n",
    "\n",
    "for ax, metric in zip(axes, metrics_to_plot):\n",
    "    vals = comparison[metric].dropna().tolist()\n",
    "    lbls = comparison.loc[comparison[metric].notna(), 'Model'].tolist()\n",
    "    bars = ax.bar(lbls, vals, color=bar_colors[:len(vals)],\n",
    "                  edgecolor='none', width=0.5)\n",
    "\n",
    "    for bar, val in zip(bars, vals):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2,\n",
    "                bar.get_height() + 0.005,\n",
    "                f'{val:.3f}', ha='center', va='bottom',\n",
    "                color='white', fontsize=10, fontweight='bold')\n",
    "\n",
    "    ax.set_title(metric, color='white', fontsize=12, fontweight='bold')\n",
    "    ax.set_facecolor('#161B22')\n",
    "    ax.tick_params(colors='white')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    for spine in ax.spines.values(): spine.set_edgecolor('#30363D')\n",
    "    ax.yaxis.label.set_color('white')\n",
    "\n",
    "plt.suptitle('Model Comparison: YOLOv5m vs YOLOv8s vs Ensemble',\n",
    "             color='white', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150,\n",
    "            bbox_inches='tight', facecolor='#0D1117')\n",
    "plt.show()\n",
    "\n",
    "# Print improvements\n",
    "v5_map  = metrics_v5.box.map50\n",
    "v8_map  = metrics_v8.box.map50\n",
    "ens_map = ensemble_stats[1]\n",
    "print(f'\\nüìä mAP@0.5 Improvements:')\n",
    "print(f'  Ensemble vs YOLOv5m : +{(ens_map - v5_map)*100:.2f}%')\n",
    "print(f'  Ensemble vs YOLOv8s : +{(ens_map - v8_map)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 12. Visualise: Single Model vs Ensemble Predictions"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(ax, image_np, boxes_norm, scores, labels, title, color_map):\n",
    "    \"\"\"Draw normalised xyxy boxes on a matplotlib axis.\"\"\"\n",
    "    h, w = image_np.shape[:2]\n",
    "    ax.imshow(image_np)\n",
    "    ax.set_title(title, color='white', fontsize=10, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    ax.set_facecolor('#161B22')\n",
    "\n",
    "    for box, score, label in zip(boxes_norm, scores, labels):\n",
    "        x1, y1, x2, y2 = box[0]*w, box[1]*h, box[2]*w, box[3]*h\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=2, edgecolor=color_map.get(int(label), 'white'),\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1-6,\n",
    "                f\"{CLASSES.get(int(label), '?')} {score:.2f}\",\n",
    "                color='white', fontsize=7, fontweight='bold',\n",
    "                bbox=dict(facecolor=color_map.get(int(label), '#333'),\n",
    "                          alpha=0.85, pad=1.5, edgecolor='none'))\n",
    "\n",
    "\n",
    "def compare_predictions(image_path, n_cols=3):\n",
    "    \"\"\"Side-by-side: YOLOv5 | YOLOv8 | Ensemble.\"\"\"\n",
    "    img_np = np.array(Image.open(image_path).convert('RGB'))\n",
    "    h, w   = img_np.shape[:2]\n",
    "\n",
    "    b5, s5, l5 = predict_single(model_v5, image_path)\n",
    "    b8, s8, l8 = predict_single(model_v8, image_path)\n",
    "    be, se, le = ensemble_predict(image_path, [model_v5, model_v8],\n",
    "                                  weights=[0.45, 0.55])\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.patch.set_facecolor('#0D1117')\n",
    "\n",
    "    draw_boxes(axes[0], img_np, b5, s5, l5, f'YOLOv5m  ({len(b5)} detections)', COLORS)\n",
    "    draw_boxes(axes[1], img_np, b8, s8, l8, f'YOLOv8s  ({len(b8)} detections)', COLORS)\n",
    "    draw_boxes(axes[2], img_np, be, se, le, f'Ensemble WBF  ({len(be)} detections)', COLORS)\n",
    "\n",
    "    plt.suptitle(f'Predictions on {Path(image_path).name}',\n",
    "                 color='white', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'comparison_{Path(image_path).stem}.png', dpi=150,\n",
    "                bbox_inches='tight', facecolor='#0D1117')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run on first 3 test images\n",
    "test_imgs = sorted((DATASET_PATH / 'images' / 'test').glob('*.jpg'))[:3]\n",
    "for img_path in test_imgs:\n",
    "    compare_predictions(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 13. Per-Class AP with Ensemble"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class AP from YOLOv5 and YOLOv8 (available directly from Ultralytics)\n",
    "class_names = list(CLASSES.values())\n",
    "\n",
    "ap_v5 = metrics_v5.box.ap50          # array of per-class AP@0.5\n",
    "ap_v8 = metrics_v8.box.ap50\n",
    "\n",
    "# Per-class ensemble AP requires custom COCO eval ‚Äî approximated as:\n",
    "# run evaluate_ensemble() with per_class=True using COCOeval catIds filter\n",
    "def per_class_ensemble_ap(models, weights, iou_thr=0.55):\n",
    "    test_img_dir = DATASET_PATH / 'images' / 'test'\n",
    "    test_lbl_dir = DATASET_PATH / 'labels' / 'test'\n",
    "    coco_gt_dict = build_coco_gt(test_img_dir, test_lbl_dir)\n",
    "    gt_path = '/tmp/coco_gt_pc.json'\n",
    "    with open(gt_path, 'w') as f: json.dump(coco_gt_dict, f)\n",
    "    coco_gt = COCO(gt_path)\n",
    "    img_list = sorted(test_img_dir.glob('*.jpg'))\n",
    "    img_name_to_id = {im['file_name']: im['id'] for im in coco_gt_dict['images']}\n",
    "\n",
    "    dt_list = []\n",
    "    for img_path in img_list:\n",
    "        img_id = img_name_to_id[img_path.name]\n",
    "        img = Image.open(img_path); W, H = img.size\n",
    "        fboxes, fscores, flabels = ensemble_predict(img_path, models, weights=weights, iou_thr=iou_thr)\n",
    "        for box, score, label in zip(fboxes, fscores, flabels):\n",
    "            x1,y1,x2,y2 = box\n",
    "            dt_list.append({'image_id': img_id, 'category_id': int(label),\n",
    "                            'bbox': [x1*W, y1*H, (x2-x1)*W, (y2-y1)*H], 'score': float(score)})\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(dt_list)\n",
    "    aps = []\n",
    "    for cat_id in range(4):\n",
    "        ev = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "        ev.params.catIds = [cat_id]\n",
    "        ev.params.iouThrs = np.array([0.5])\n",
    "        ev.evaluate(); ev.accumulate()\n",
    "        # stats[1] = AP@0.5\n",
    "        ev.summarize()\n",
    "        aps.append(ev.stats[0])\n",
    "    return aps\n",
    "\n",
    "print('Computing per-class AP for ensemble...')\n",
    "ap_ensemble = per_class_ensemble_ap([model_v5, model_v8], weights=[0.45, 0.55])\n",
    "\n",
    "# Plot grouped bar chart\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "fig.patch.set_facecolor('#0D1117')\n",
    "ax.set_facecolor('#161B22')\n",
    "\n",
    "r1 = ax.bar(x - width, ap_v5,       width, label='YOLOv5m',       color='#F5A623', edgecolor='none')\n",
    "r2 = ax.bar(x,         ap_v8,       width, label='YOLOv8s',       color='#3B82F6', edgecolor='none')\n",
    "r3 = ax.bar(x + width, ap_ensemble, width, label='Ensemble (WBF)', color='#22C55E', edgecolor='none')\n",
    "\n",
    "for bars in [r1, r2, r3]:\n",
    "    for bar in bars:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2,\n",
    "                bar.get_height() + 0.01,\n",
    "                f'{bar.get_height():.2f}',\n",
    "                ha='center', va='bottom', color='white', fontsize=8)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names, color='white', fontsize=11)\n",
    "ax.set_ylabel('AP@0.5', color='white')\n",
    "ax.set_title('Per-Class AP@0.5: YOLOv5m vs YOLOv8s vs Ensemble',\n",
    "             color='white', fontsize=12, fontweight='bold')\n",
    "ax.tick_params(colors='white')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(facecolor='#1C2128', labelcolor='white', fontsize=10)\n",
    "for spine in ax.spines.values(): spine.set_edgecolor('#30363D')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_ap.png', dpi=150, bbox_inches='tight', facecolor='#0D1117')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 14. WBF Weight Sensitivity Analysis"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep different weight ratios for v5 vs v8 to find the optimal split\n",
    "weight_ratios = [(1.0, 0.0), (0.7, 0.3), (0.55, 0.45),\n",
    "                 (0.5, 0.5), (0.45, 0.55), (0.3, 0.7), (0.0, 1.0)]\n",
    "labels_wr = [f'v5={a:.1f}\\nv8={b:.1f}' for a, b in weight_ratios]\n",
    "maps_wr = []\n",
    "\n",
    "for w5, w8 in weight_ratios:\n",
    "    if w5 == 1.0:\n",
    "        maps_wr.append(metrics_v5.box.map50)\n",
    "    elif w8 == 1.0:\n",
    "        maps_wr.append(metrics_v8.box.map50)\n",
    "    else:\n",
    "        stats = evaluate_ensemble([model_v5, model_v8], weights=[w5, w8])\n",
    "        maps_wr.append(stats[1])\n",
    "    print(f'  w=({w5:.2f},{w8:.2f}) ‚Üí mAP@0.5 = {maps_wr[-1]:.4f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 4))\n",
    "fig.patch.set_facecolor('#0D1117')\n",
    "ax.set_facecolor('#161B22')\n",
    "\n",
    "bars = ax.bar(labels_wr, maps_wr, color='#3B82F6', edgecolor='none', width=0.5)\n",
    "\n",
    "best_idx = np.argmax(maps_wr)\n",
    "bars[best_idx].set_color('#22C55E')\n",
    "\n",
    "for bar, val in zip(bars, maps_wr):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.003,\n",
    "            f'{val:.3f}', ha='center', va='bottom',\n",
    "            color='white', fontsize=8, fontweight='bold')\n",
    "\n",
    "ax.axhline(metrics_v5.box.map50, color='#F5A623', linestyle='--', linewidth=1.2, label='YOLOv5m alone')\n",
    "ax.axhline(metrics_v8.box.map50, color='#3B82F6', linestyle='--', linewidth=1.2, label='YOLOv8s alone')\n",
    "\n",
    "ax.set_ylabel('mAP@0.5', color='white')\n",
    "ax.set_title('WBF Weight Sensitivity Analysis', color='white', fontsize=12, fontweight='bold')\n",
    "ax.tick_params(colors='white')\n",
    "ax.set_ylim(min(maps_wr) - 0.02, 1.0)\n",
    "ax.legend(facecolor='#1C2128', labelcolor='white')\n",
    "for spine in ax.spines.values(): spine.set_edgecolor('#30363D')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wbf_weight_sensitivity.png', dpi=150, bbox_inches='tight', facecolor='#0D1117')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nüèÜ Best weight ratio: v5={weight_ratios[best_idx][0]:.2f}, v8={weight_ratios[best_idx][1]:.2f}')\n",
    "print(f'   Best mAP@0.5 = {maps_wr[best_idx]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 15. Final Summary"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame([\n",
    "    {'Model': 'YOLOv5m',        'mAP@0.5': metrics_v5.box.map50,  'mAP@0.5:0.95': metrics_v5.box.map,  'Precision': metrics_v5.box.mp, 'Recall': metrics_v5.box.mr},\n",
    "    {'Model': 'YOLOv8s',        'mAP@0.5': metrics_v8.box.map50,  'mAP@0.5:0.95': metrics_v8.box.map,  'Precision': metrics_v8.box.mp, 'Recall': metrics_v8.box.mr},\n",
    "    {'Model': 'Ensemble (WBF)', 'mAP@0.5': ensemble_stats[1],     'mAP@0.5:0.95': ensemble_stats[0],   'Precision': None,              'Recall': None},\n",
    "])\n",
    "\n",
    "print('=' * 65)\n",
    "print('              FINAL RESULTS SUMMARY')\n",
    "print('=' * 65)\n",
    "print(summary.to_string(index=False, float_format='{:.4f}'.format))\n",
    "print('=' * 65)\n",
    "\n",
    "best_single = max(metrics_v5.box.map50, metrics_v8.box.map50)\n",
    "gain = (ensemble_stats[1] - best_single) * 100\n",
    "print(f'\\nüöÄ Ensemble improvement over best single model: +{gain:.2f}% mAP@0.5')\n",
    "print(f'   No additional training required ‚Äî inference-time fusion only.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
